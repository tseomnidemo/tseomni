# Abstract

Human perception naturally integrates both auditory and visual cues; however, most target speech extraction (TSE) models have not fully embraced the two informative cues. To bridge this gap, we propose TSE-Omni, a cognitively inspired model that unifies audio and visual TSE tasks within a single autoregressive large language model (AR-LLM). With a visual cue integration module, TSE-Omni supports either pre-registered audio cues or synchronous visual cues.  With the inherent next-token prediction capability of LLM, the AR-LLM leverages historical speech extraction tokens in a ``self-enrollment" manner, and integrates seamlessly with temporally synchronized visual tokens. Experimental results suggest that TSE-Omni maintains robustness when visual cues are corrupted. With its unified nature and lightweight architecture, TSE-Omni significantly reduces deployment costs while maintaining strong performance. Demo:[](https://tseomnidemo.github.io/tseomni/)
